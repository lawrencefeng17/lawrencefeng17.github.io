---
layout: narrow
title: "Alignment Reading Group: March 21, 2025"
---
# Agenda for March 21, 2025: Robustness, Unlearning, and Control

* **Adversarial robustness** attempts to understand and defend against adversarial inputs
* **Machine unlearning** attempts to remove data or capabilities from a model
* **AI Control** sets up scaffolding around AI systems so that we can detect and catch deception and other unsafe behavior

## 1. Core Content and Discussions

### 1.1 [Weak-to-Strong Generalization](https://arxiv.org/pdf/2312.09390) from last week (30 minutes)

### 1.2 [What is AI Adversarial Robustness?](https://research.ibm.com/blog/securing-ai-workflows-with-adversarial-robustness) (10 minutes)

### 1.3 [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://llm-attacks.org/) (10 minutes)
* Read the overview and example sections, not the paper itself

### 1.4 [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/pdf/1902.02918) (10 minutes)

### 1.5 [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/pdf/2403.03218#page=8) (30 minutes)
* Read sections 4, 5.2, 5.3 and Appendix B6 only

### 1.6 [AI Control: Improving Safety Despite Intentional Subversion](https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion) (20 minutes) 