---
layout: narrow
title: "Alignment Reading Group: March 21, 2025"
---
# Robustness, Unlearning, and Control

* **Adversarial robustness** attempts to understand and defend against adversarial inputs
* **Machine unlearning** attempts to remove data or capabilities from a model
* **AI Control** sets up scaffolding around AI systems so that we can detect and catch deception and other unsafe behavior

## 1. Core Content and Discussions

### 1.1 [Weak-to-Strong Generalization](https://arxiv.org/pdf/2312.09390) (20 minutes)
* ...from last week

### 1.2 [What is AI Adversarial Robustness?](https://research.ibm.com/blog/securing-ai-workflows-with-adversarial-robustness) (10 minutes)
* An overview of adversarial robustness

### 1.3 [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://llm-attacks.org/) (10 minutes)
* Read the overview and example sections, not the paper itself

### 1.4 [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/pdf/1902.02918) (if time permits)

### 1.5 [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/pdf/2403.03218#page=8) (30 minutes)
* Read sections 4, 5.2, 5.3 and Appendix B6 only

### 1.6 [AI Control: Improving Safety Despite Intentional Subversion](https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion) (20 minutes) 
* AI control explores setups that reduce risks from advanced AI systems, even when models might be intentionally deceptive.
* This forum post introduces AI control, and reports on the results of trying various protocols: both in terms of usefulness (how much performance of the model are you recovering) and safety (how often can the model get past the safeguards).