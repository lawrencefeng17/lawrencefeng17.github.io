---
layout: post
title: AI Safety Notes
subtitle: from a reading group I'm in
thumbnail-img: "/assets/img/casi.png"
tags: [Notes]
author: Lawrence Feng
---

I'm part of an AI Safety reading group here at Carnegie Mellon, and I thought I'd type up my notes and thoughts on some of our readings and share them here. Assume any figures I use are not my own.

---
## [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073)
[Reinforcement learning from human feedback](https://arxiv.org/pdf/1706.03741) (RLHF) has become an established method of aligning AI models to human values. However, there may be [fundamental limitations to RLHF](https://arxiv.org/pdf/2307.15217): For example, obtaining quality human feedback in large quantities is difficult; humans can themselves be misaligned, whether intentional or unintentional. Furthermore, as AI models become more capable, and perhaps more intelligent than ourselves, we cannot rely on human supervision to align our AI systems; AI performance may exceed human performance, and so we will need other ways to evaluate, oversee, and align. That is, we need to be able to scale supervision as a part of addressing the superalignment problem. The authors also wanted to address the tension between helpfulness and harmlessness. For example, a model may always reply with ":)," which is very much harmless, but unfortunately it is completely unhelpful. 
Researchers at Anthropic developed a method they call Constitutional AI, aligning a AI model without any human feedback. Instead, the authors wrote a "constitution" of 10 principles in natural natural language, chosen fairly ad hoc for the purposes of this research. 
![Their approach, taken from the paper](/Users/lfeng/Projects/lawrencefeng17.github.io/assets/img/constitutional.png)

- **Approach**: There are two stages to this extreme form of scaled supervision without human feedback.
    - (Supervised stage): Begin by prompting a helpful-only AI assistant with prompts that elicit harmful behaviors. The model is asked to critique and revise its response with respect to the provided constitution. Then, a pretrained language model is fine-tuned using supervised learning on these final revised responses. 
        - 





---
## [WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION](https://arxiv.org/pdf/2312.09390.pdf)
This paper considers if and how we can align and supervise superhuman models. Of course, we do not yet have superhuman models, so the paper seeks to test if *weak models* can effectively supervise *strong models*. The authors experiment if GPT-2 trained on ground truth can act as a supervisor for GPT-4. They found that it can. Fine-tuning using a GPT-2-level supervisor and an auxiliary confidence loss can produce a GPT-3.5-level performance.

- Scalable oversight is a big topic in alignment. The more intelligent and complex models get, the harder it is for humans to reliably evaluate the output of these models. By definition, a superhuman model would generate outputs beyond the capabilities of humans. In the event that we develop superhuman AI, we still want to be able to effectively and efficiently align these models.
- **Question**: Humans will be *weak* supervisors compared to superhuman AI. Can we use a weak model to supervise a strong model?
- **Intuition**: We don't want a stronger model to imitate a weaker model. After all, the stronger model is more capable, more intelligent, and more knowledgeable; we do not want the stronger model to behave like the weaker model. When we generate labeled data using a fine-tuned, weak model, and use these labels as fine-tuning data for the stronger model, we hope that the stronger model already contains the capabilities to correctly perform the tasks we care about, and so instead of imitating the incorrect output from the weaker, fine-tuned model, the stronger model should pick up on the aligned behavior of the weaker, fine-tuned model. Therefore, we will hope to produce a stronger, aligned model. 
- **Method**: We generate *weak labels* by taking the outputs of a fine-tuned *weak model*. Now, let us fine-tune a strong model with the data labeled by our weak model. Because we don't have superhuman AI, we can fine-tune our strong model with the ground truth and use it as comparison. The authors operationalize the intuition that the student (the stronger model) should learn the intent of the supervisor (the weaker model), but not imitate the incorrect behavior of the weaker supervisor, using auxiliary confidence loss term. When a strong model's output differs from the weak model's label, the additional loss term reinforces the strong model's confidence in its own answer.


