---
layout: post
title: "Training LLMs using RL: A Credit Assignment Perspective"
tags: [Projects]
author: Lawrence Feng
---

What this post covers:
* n-step TD, GAE, credit assignment and variance reduction in traditional RL
* How is RL of LLMs different than tradition RL?
* PPO, GRPO, DPO 
...

## A story
Suppose you took a fifth grader and decided you wanted to make her an expert mathematician. Astonishingly, the following method works surprisingly well in practice.

Give her many problems, and ask her to write an answer in the blank. A machine checks the answer in the blank, marking it as right or wrong, returning a single scalar score. We don’t grade any of the work written outside the blank. No partial credit. (Maybe we add a small extra penalty if she refuses to write anything in the blank at all.)

Now we’re going to surgically improve her by making tiny edits to her neurons—the little computational units inside the brain, roughly speaking. After she writes an answer, we ask a very specific question: if we could nudge each neuron a little bit, which nudge would make it more likely that she would produce exactly the answer she just wrote, given the same problem?

If her answer was right, we nudge each neuron in the direction that makes this behavior more likely to happen again. If her answer was wrong, we do the opposite, nudging each neuron in the direction that makes this behavior less likely.

> To me, this feels completely unnatural. How could this possibly work?

The machine only tells us whether the final answer is right or wrong. It doesn’t tell us which part of her scratch work was insightful, which step was unnecessary, or where she first went off track. It doesn’t even tell us whether the wrong answer was “almost right,” or wrong for a subtle reason. It gives us one number, and we’re going to use that to alter her entire brain.

What I’ve illustrated here isn’t that far off from how modern language models—such as DeepSeek-R1 or Qwen3—are improved when they’re trained to reason about math or code. In these stages, models generate a complete solution, receive a score only at the end (often from a verifier or reward model), and are updated using that outcome signal. Despite how blunt this feedback is, this approach is precisely what pushed these models to expert-level performance on math and coding benchmarks.

In this post, I’ll explain how we arrive at such an approach. We’ll revisit familiar reinforcement learning ideas—policy gradients, returns, baselines, and advantage estimation—but from the perspective of language models trained with outcome-only feedback. Rather than starting from the full RL formalism, I’ll use credit assignment as the organizing principle, and show how methods like PG, PPO, GRPO-style objectives, and DPO can be understood as different answers to the same underlying question: how do we make a single outcome-level judgment meaningfully shape a long chain of decisions?

### From a single score to many decisions

Let’s now make the setup precise.

We have a policy $\pi_\theta(a \mid s)$, parameterized by $\theta$, interacting with an environment and producing a trajectory
$$
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_T, a_T, r_T).
$$

Our goal is to adjust $\theta$ so that the policy produces *better* trajectories. As in most of modern deep learning, we will do this using **first-order optimization**: we compute the gradient of some objective with respect to the parameters, and then move the parameters slightly in the direction suggested by that gradient (possibly using an optimizer that rescales or smooths the update).

In policy gradient methods, the objective we optimize is the **expected return**
$$
J(\theta) \;=\; \mathbb{E}_{\tau \sim \pi_\theta}\big[ R(\tau) \big],
\qquad
R(\tau) = \sum_{t=0}^T \gamma^t r_t.
$$

Here, $\gamma \in (0,1]$ is the **discount factor**, which controls how much we value future rewards relative to immediate ones. When $\gamma = 1$, all rewards are weighted equally; when $\gamma < 1$, rewards further in the future contribute less to the total return.

> **You can read this as:**  
> The expected return $J(\theta)$ is the average total reward we would obtain if we repeatedly generated trajectories $\tau$ by following the policy $\pi_\theta$ (together with the environment’s transition dynamics), summing up the rewards along each trajectory with a preference for earlier rewards when $\gamma < 1$.

Already, the credit assignment problem is visible.  
$R(\tau)$ is a *single scalar*, but the trajectory contains a long sequence of actions. Somehow, that one number must be used to update every decision that produced it.

---

### Differentiating through behavior, not rewards

The reward function itself is not differentiable with respect to $\theta$. The key idea behind policy gradients is to instead differentiate **how likely the trajectory was under the policy**.

We start by taking the gradient of the objective:
$$
\nabla_\theta J(\theta)
= \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)].
$$

Writing the expectation explicitly,
$$
= \nabla_\theta \int p_\theta(\tau)\, R(\tau)\, d\tau.
$$

> **You can read this as:**  
> We are averaging the return $R(\tau)$ over all possible trajectories, weighted by how likely each trajectory is under the current policy parameters $\theta$.

We now move the gradient inside the integral:
$$
= \int \nabla_\theta p_\theta(\tau)\, R(\tau)\, d\tau.
$$

At this point we apply the identity
$$
\nabla_\theta p_\theta(\tau) = p_\theta(\tau)\, \nabla_\theta \log p_\theta(\tau),
$$
often called the **log-derivative trick**.

> **Intuitively:**  
> Instead of asking “how does the probability of this trajectory change?”, we ask “how does the *log-probability* change?”, and then weight that change by how good the trajectory was.

This gives
$$
= \int p_\theta(\tau)\, \nabla_\theta \log p_\theta(\tau)\, R(\tau)\, d\tau
= \mathbb{E}_{\tau \sim \pi_\theta}\big[\nabla_\theta \log p_\theta(\tau)\, R(\tau)\big].
$$

This step is the mathematical heart of policy gradients: the gradient of expected reward becomes an expectation of a gradient.

---

### Breaking the trajectory apart

The probability of a trajectory factorizes as
$$
p_\theta(\tau)
= p(s_0)\prod_{t=0}^T \pi_\theta(a_t \mid s_t)\, p(s_{t+1} \mid s_t, a_t).
$$

> **You can read this as:**  
> A trajectory is generated by sampling an initial state, then repeatedly sampling an action from the policy and a next state from the environment’s transition dynamics.

Only the policy $\pi_\theta$ depends on $\theta$. The environment dynamics do not. As a result,
$$
\nabla_\theta \log p_\theta(\tau)
= \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t \mid s_t).
$$

Substituting this back into the gradient expression,
$$
\nabla_\theta J(\theta)
= \mathbb{E}\left[
\sum_{t=0}^T
\nabla_\theta \log \pi_\theta(a_t \mid s_t)\; R(\tau)
\right].
$$

This equation already exposes the core difficulty:

- Each action contributes a term $\nabla_\theta \log \pi_\theta(a_t \mid s_t)$, describing how to make that action more likely.
- Every one of those terms is multiplied by the *same scalar* $R(\tau)$.

At this stage, all actions in the trajectory receive identical credit or blame. The estimator is unbiased—but extremely noisy.

---

### Returns-to-go: introducing temporal structure

We can improve credit assignment by recognizing that an action should not be credited for rewards that occurred *before* it was taken.

Define the **return-to-go**
$$
G_t = \sum_{k=t}^T \gamma^{k-t} r_k.
$$

> **You can read this as:**  
> $G_t$ is the total (discounted) reward accumulated *after* time step $t$.

Replacing $R(\tau)$ with $G_t$ yields
$$
\nabla_\theta J(\theta)
= \mathbb{E}\left[
\sum_{t=0}^T
\nabla_\theta \log \pi_\theta(a_t \mid s_t)\; G_t
\right].
$$

This estimator is commonly referred to as **REINFORCE**. It introduces a minimal form of temporal credit assignment: later rewards matter more for later actions.

In language-model reinforcement learning with outcome-only rewards, however, all $G_t$ are nearly identical. Every token inherits essentially the same final score.

---

### Baselines and advantages: credit relative to expectation

Even with returns-to-go, variance grows rapidly with long trajectories. A crucial observation is that we can subtract a baseline $b(s_t)$ without changing the expected gradient:
$$
\mathbb{E}\big[\nabla_\theta \log \pi_\theta(a_t \mid s_t)\, b(s_t)\big] = 0.
$$

This leads to
$$
\nabla_\theta J(\theta)
= \mathbb{E}\left[
\sum_{t=0}^T
\nabla_\theta \log \pi_\theta(a_t \mid s_t)\; \big(G_t - b(s_t)\big)
\right].
$$

A particularly useful choice is $b(s_t) = V^\pi(s_t)$, the value function. This defines the **advantage**
$$
A_t = G_t - V^\pi(s_t).
$$

> **You can read this as:**  
> The advantage measures how much better or worse the observed outcome was compared to what the policy expected at that state.

The gradient now becomes
$$
\nabla_\theta J(\theta)
= \mathbb{E}\left[
\sum_{t=0}^T
\nabla_\theta \log \pi_\theta(a_t \mid s_t)\; A_t
\right].
$$

This is the canonical policy gradient form used in modern algorithms.

---

### Why this framing matters

At this point, the core difficulties are already visible:

- **Delayed rewards** force many decisions to share a single learning signal.
- **Long horizons** amplify variance.
- **Outcome-only supervision**, common in language models, makes credit assignment especially blunt.
- **Baselines and advantages** are early attempts to make that signal more precise without introducing bias.

Everything that follows—generalized advantage estimation, PPO’s clipping and KL control, group-relative objectives, and preference-based methods like DPO—can be understood as progressively more careful ways of answering the same question:

> *How do we turn a single outcome-level judgment into meaningful feedback for a long sequence of decisions?*

This is the perspective we’ll keep as we move forward.
