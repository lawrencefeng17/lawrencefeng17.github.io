---
layout: post
title: Solving Jigsaw Puzzles using Reinforcement Learning
subtitle: Inspired by AlphaZero
thumbnail-img: "/assets/img/puzzle-785.png"
gh-repo: james-ngai/11-785-Project
tags: [Projects]
author: Lawrence Feng
---

## How it Started (the inspiration)

This project began from a desire to meld reinforcement learning with image manipulation. Convolutional neural networks (CNN) can perform position-invariant classification (i.e. a CNN can identify a flower in an image no matter where the flower is in the image).

But what if we wanted to perform a different sort of *something*-invariant classification? For example, classification invariant under corruption, scrambling, etc. We want to be able to extract features from an image under variation so that we are still able to perform classification. A related area of research is called dynamic feature selection or active feature acquisition. 

## How it Went (what we did)

As with many projects, we began with big ideas and high aspirations. However, due to limited experience with reinforcement learning and very limited time, we settled for a simpler problem that was still educationally valuable and interesting.

We developed an algorithm employing deep reinforcement learning to perform reassembly on scrambled jigsaw puzzles of the MNIST dataset. 

Specifically, we took MNIST images and created 3x3 jigsaw puzzles out of them. We framed this jigsaw game as a sequential swapping game, where each move involved swapping one piece with another. The goal of our algorithm was to put the jigsaw together in the least number of swaps (correctness and efficiency).

## Architecture (some details)

We were loosely inspired by [AlphaZero](https://rdcu.be/dEhq4) (Silver et al. 2016), though we made significant modifications for our specific game and did not use self-play.

We trained a value network using supervised learning. The network took in a jigsaw puzzle (perhaps partially reassembled), and it outputted a scalar from 0 to 1 describing how close the puzzle was to being fully reassembled.

We trained a policy network using reinforcement learning. The network took in a jigsaw puzzle (perhaps partially reassembled),  and it outputted a set of action-probability pairs.

Then, we performed Monte Carlo Tree Search with PUCT that combined the value network and the policy network to perform depth reduction and breadth reduction, respectively. The guiding principle is that the space of all possible sequences of swaps is too large to be searched efficiently, so we use deep learning to perform a more intelligent search.

![Architecture](/assets/img/jigsaw-architecture.png)
---
#### Summary video of what we did!

[Project video](https://www.youtube.com/watch?v=DpJcMY3AIuo&ab_channel=LawrenceFeng)

#### Code

[Repo](https://github.com/james-ngai/11-785-Project)
